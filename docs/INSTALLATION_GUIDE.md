# üìö ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Medical AI

## üéØ Overview

‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô Medical AI Q&A System ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ **Langchain** ‡πÅ‡∏•‡∏∞ **N8N** ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ **FastAPI** ‡πÄ‡∏õ‡πá‡∏ô optional API layer

### ‚≠ê Core Components
- **Langchain** - ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
- **N8N** - Workflow orchestration ‡πÅ‡∏•‡∏∞ automation
- **FastAPI** - Optional REST API wrapper
- **ChromaDB** - Vector database ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
- **SQLite** - Database ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging ‡πÅ‡∏•‡∏∞ analytics

---

## üöÄ ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Requirements

**System Requirements:**
- Python 3.8+
- Docker & Docker Compose
- Git
- ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 8GB RAM (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI models)
- ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 10GB disk space

**‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô:**
```bash
python --version
docker --version
docker-compose --version
git --version
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Clone ‡πÅ‡∏•‡∏∞ Setup Environment

```bash
# Clone repository
git clone <your-repo-url> medical-ai-system
cd medical-ai-system

# ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment
python -m venv .venv

# Activate environment (Windows)
.\.venv\Scripts\activate

# Activate environment (Linux/Mac)
source .venv/bin/activate

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
pip install -r requirements.txt
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏≤‡∏£ Config ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô

**‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:**
```bash
mkdir -p data/{raw,processed,exports}
mkdir -p models/{trained,cache}
mkdir -p logs
```

**‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ environment variables:**
```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á .env file
echo "ENVIRONMENT=development" > .env
echo "LOG_LEVEL=info" >> .env
echo "MODEL_CACHE_DIR=./models/cache" >> .env
echo "CHROMA_DB_PATH=./data/vectorstore" >> .env
```

---

## üß† Langchain Service Setup

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

**1. ‡∏£‡∏±‡∏ô Langchain Service ‡πÅ‡∏ö‡∏ö Standalone:**
```bash
python run_langchain.py
```

**2. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Output:**
```
üè• Initializing Langchain Medical AI Service...
ü§ñ Loading default medical model...
üìö Initializing ChromaDB vector store...
üîß Setting up RetrievalQA chain...
‚úÖ Langchain Medical Service ready!

üí¨ Interactive Mode - ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠ 'quit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
ü§ñ AI: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏Å‡∏£‡∏∞‡∏´‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏≤‡∏Å ‡∏õ‡∏±‡∏™‡∏™‡∏≤‡∏ß‡∏∞‡∏ö‡πà‡∏≠‡∏¢ ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏•‡∏î ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏á‡πà‡∏≤‡∏¢
üìä Confidence: 0.87 | Sources: ['medical_knowledge_base']
```

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Custom Model

**1. ‡∏ß‡∏≤‡∏á Model Files:**
```
models/
‚îú‚îÄ‚îÄ my_medical_model/
‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îî‚îÄ‚îÄ special_tokens_map.json
```

**2. ‡∏£‡∏±‡∏ô Service ‡∏î‡πâ‡∏ß‡∏¢ Custom Model:**
```bash
python run_langchain.py --model-path ./models/my_medical_model
```

### ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÉ‡∏´‡∏°‡πà

**‡∏ú‡πà‡∏≤‡∏ô Python API:**
```python
from langchain_service.medical_ai import LangchainMedicalService

service = LangchainMedicalService()

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÜ
service.add_knowledge(
    content="‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•‡πÉ‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏™‡∏π‡∏á...",
    metadata={"topic": "diabetes", "source": "medical_textbook"}
)

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
service.add_knowledge_from_file("data/medical_articles.txt")
```

**‡∏ú‡πà‡∏≤‡∏ô Batch Script:**
```bash
# ‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô data/raw/
# ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏£‡∏±‡∏ô
python tools/data_processing/add_knowledge.py
```

---

## üåê FastAPI Integration

### ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô FastAPI Server

**1. ‡∏£‡∏±‡∏ô FastAPI ‡πÅ‡∏ö‡∏ö Development:**
```bash
cd fastapi/app
python main.py
```

**2. ‡∏£‡∏±‡∏ô FastAPI ‡πÅ‡∏ö‡∏ö Production:**
```bash
uvicorn fastapi.app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### API Endpoints Documentation

**Base URL:** `http://localhost:8000`

#### 1. Health Check
```bash
GET /health
```
**Response:**
```json
{
  "status": "healthy",
  "langchain_service": "connected",
  "stats": {
    "questions_answered": 145,
    "knowledge_base_size": 1250,
    "avg_confidence": 0.82
  },
  "timestamp": "2025-09-15T14:30:15Z"
}
```

#### 2. Ask Medical Question
```bash
POST /api/medical-qa
Content-Type: application/json

{
  "question": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
  "user_id": "user123"
}
```
**Response:**
```json
{
  "answer": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏Å‡∏£‡∏∞‡∏´‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏≤‡∏Å ‡∏õ‡∏±‡∏™‡∏™‡∏≤‡∏ß‡∏∞‡∏ö‡πà‡∏≠‡∏¢ ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏•‡∏î ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏á‡πà‡∏≤‡∏¢",
  "confidence": 0.87,
  "sources": ["medical_knowledge_base"],
  "response_time": 0.45,
  "status": "success"
}
```

#### 3. Add Knowledge
```bash
POST /api/add-knowledge
Content-Type: application/json

{
  "content": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÉ‡∏´‡∏°‡πà...",
  "topic": "diabetes",
  "category": "symptoms"
}
```

#### 4. Get Statistics
```bash
GET /api/stats
```

#### 5. Test Endpoint
```bash
POST /api/test?question=‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
```

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô API Documentation
‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà `http://localhost:8000/docs` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Interactive API Documentation (Swagger UI)

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö API

**‡∏ú‡πà‡∏≤‡∏ô cURL:**
```bash
# Health check
curl http://localhost:8000/health

# Ask question
curl -X POST "http://localhost:8000/api/medical-qa" \
  -H "Content-Type: application/json" \
  -d '{"question": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?", "user_id": "test"}'

# Add knowledge
curl -X POST "http://localhost:8000/api/add-knowledge" \
  -H "Content-Type: application/json" \
  -d '{"content": "‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏£‡∏Ñ...", "topic": "diabetes", "category": "info"}'
```

**‡∏ú‡πà‡∏≤‡∏ô Python:**
```python
import requests

# Ask question
response = requests.post(
    "http://localhost:8000/api/medical-qa",
    json={"question": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?", "user_id": "python_test"}
)
print(response.json())

# Health check
health = requests.get("http://localhost:8000/health")
print(health.json())
```

---

## üîÑ N8N Workflow Integration

### ‡∏Å‡∏≤‡∏£ Setup N8N

**1. ‡∏£‡∏±‡∏ô N8N Server:**
```bash
# ‡πÅ‡∏ö‡∏ö Standalone
npx n8n

# ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡πà‡∏≤‡∏ô Docker
docker run -it --rm \
  --name n8n \
  -p 5678:5678 \
  -v n8n_data:/home/node/.n8n \
  n8nio/n8n
```

**2. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô N8N:**
- ‡πÄ‡∏õ‡∏¥‡∏î `http://localhost:5678`
- ‡∏™‡∏£‡πâ‡∏≤‡∏á account ‡πÅ‡∏£‡∏Å
- Login ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö

### Import Medical AI Workflow

**1. Import Workflow JSON:**
```bash
# Copy workflow file
cp n8n_workflows/medical_qa_workflow.json /path/to/n8n/workflows/

# ‡∏´‡∏£‡∏∑‡∏≠ import ‡∏ú‡πà‡∏≤‡∏ô N8N UI
```

**2. Workflow Components:**
- **Webhook Node** - ‡∏£‡∏±‡∏ö HTTP requests
- **HTTP Request Node** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å FastAPI endpoint
- **Code Node** - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• response
- **Database Node** - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Workflow

**1. Medical Q&A Workflow:**
```json
{
  "nodes": [
    {
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "parameters": {
        "path": "medical-qa",
        "httpMethod": "POST"
      }
    },
    {
      "name": "Call Langchain API",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://localhost:8000/api/medical-qa",
        "method": "POST",
        "body": {
          "question": "={{ $json.question }}",
          "user_id": "={{ $json.user_id }}"
        }
      }
    },
    {
      "name": "Process Response",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "return [{ json: { ...items[0].json, processed_at: new Date().toISOString() } }];"
      }
    }
  ]
}
```

**2. ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Workflow:**
```bash
curl -X POST "http://localhost:5678/webhook/medical-qa" \
  -H "Content-Type: application/json" \
  -d '{"question": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?", "user_id": "n8n_test"}'
```

---

## üìä Database ‡πÅ‡∏•‡∏∞ Logging

### SQLite Database Structure

**Tables:**
```sql
-- Q&A Logging
CREATE TABLE medical_qa_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    question TEXT NOT NULL,
    answer TEXT,
    confidence REAL,
    sources TEXT,  -- JSON array
    response_time REAL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Knowledge Base Metadata
CREATE TABLE knowledge_metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    content_hash TEXT UNIQUE,
    topic TEXT,
    category TEXT,
    source TEXT,
    added_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### ‡∏Å‡∏≤‡∏£‡∏î‡∏π Database

**1. ‡∏ú‡πà‡∏≤‡∏ô SQLite CLI:**
```bash
sqlite3 data/medical_ai.db

-- ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Q&A ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
SELECT * FROM medical_qa_log ORDER BY created_at DESC LIMIT 10;

-- ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
SELECT 
    COUNT(*) as total_questions,
    AVG(confidence) as avg_confidence,
    COUNT(DISTINCT user_id) as unique_users
FROM medical_qa_log;
```

**2. ‡∏ú‡πà‡∏≤‡∏ô Python:**
```python
import sqlite3
import pandas as pd

conn = sqlite3.connect('data/medical_ai.db')

# Load data ‡πÄ‡∏õ‡πá‡∏ô DataFrame
df = pd.read_sql_query("""
    SELECT user_id, question, answer, confidence, created_at 
    FROM medical_qa_log 
    ORDER BY created_at DESC 
    LIMIT 100
""", conn)

print(df.head())
conn.close()
```

---

## üê≥ Docker Deployment

### Docker Compose Setup

**1. ‡∏£‡∏±‡∏ô Docker Services:**
```bash
# Start all services
docker-compose up -d

# ‡∏î‡∏π status
docker-compose ps

# ‡∏î‡∏π logs
docker-compose logs -f langchain-medical
```

**2. Services ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏±‡∏ô:**
- **langchain-medical** - Main Langchain service (port 8000)
- **n8n** - Workflow orchestration (port 5678)  
- **webapp** - Optional web interface (port 80)

### ‡∏Å‡∏≤‡∏£ Build Custom Image

**1. Build Langchain Service:**
```bash
docker build -t medical-ai-langchain .
```

**2. Run Custom Container:**
```bash
docker run -d \
  --name medical-ai \
  -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  medical-ai-langchain
```

### Production Deployment

**1. Environment Variables:**
```bash
export ENVIRONMENT=production
export LOG_LEVEL=warning
export MODEL_CACHE_DIR=/app/models/cache
export CHROMA_DB_PATH=/app/data/vectorstore
```

**2. Production Docker Compose:**
```yaml
version: '3.8'
services:
  langchain-medical:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=warning
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - langchain-medical
    restart: unless-stopped
```

---

## üîß Configuration ‡πÅ‡∏•‡∏∞ Customization

### Langchain Configuration

**File:** `config/langchain_config.json`
```json
{
  "model": {
    "type": "huggingface",
    "model_name": "microsoft/DialoGPT-medium",
    "custom_model_path": null,
    "max_length": 512,
    "temperature": 0.7
  },
  "vectorstore": {
    "type": "chroma",
    "persist_directory": "./data/vectorstore",
    "collection_name": "medical_knowledge",
    "chunk_size": 1000,
    "chunk_overlap": 200
  },
  "retrieval": {
    "search_type": "similarity",
    "k": 5,
    "score_threshold": 0.7
  },
  "logging": {
    "level": "INFO",
    "file": "logs/langchain.log",
    "max_size": "10MB",
    "backup_count": 5
  }
}
```

### FastAPI Configuration

**Environment Variables:**
```bash
# Server settings
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000
FASTAPI_WORKERS=4

# CORS settings
CORS_ORIGINS=["*"]
CORS_METHODS=["GET", "POST"]

# Rate limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60
```

### Model Performance Tuning

**1. Memory Optimization:**
```python
# ‡πÉ‡∏ô langchain_service/medical_ai.py
TORCH_SETTINGS = {
    "torch_dtype": "float16",  # ‡∏•‡∏î memory usage
    "device_map": "auto",      # automatic GPU allocation
    "low_cpu_mem_usage": True  # optimize CPU memory
}
```

**2. Batch Processing:**
```python
# ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
def batch_ask_questions(questions: List[str], batch_size: int = 4):
    results = []
    for i in range(0, len(questions), batch_size):
        batch = questions[i:i+batch_size]
        batch_results = [service.ask_question(q) for q in batch]
        results.extend(batch_results)
    return results
```

---

## üîç Monitoring ‡πÅ‡∏•‡∏∞ Analytics

### Log Analysis

**1. ‡∏î‡∏π Logs ‡πÅ‡∏ö‡∏ö Real-time:**
```bash
# Langchain service logs
tail -f logs/langchain.log

# FastAPI logs
tail -f logs/fastapi.log

# Docker logs
docker-compose logs -f langchain-medical
```

**2. Log Analysis Script:**
```python
import pandas as pd
import json
from datetime import datetime, timedelta

def analyze_logs(log_file="logs/langchain.log", days=7):
    # Load ‡πÅ‡∏•‡∏∞ analyze logs
    with open(log_file, 'r') as f:
        logs = [json.loads(line) for line in f if line.strip()]
    
    df = pd.DataFrame(logs)
    
    # Filter last N days
    cutoff = datetime.now() - timedelta(days=days)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    recent_df = df[df['timestamp'] >= cutoff]
    
    # Analysis
    stats = {
        "total_questions": len(recent_df),
        "avg_confidence": recent_df['confidence'].mean(),
        "avg_response_time": recent_df['response_time'].mean(),
        "top_topics": recent_df['topic'].value_counts().head(10).to_dict()
    }
    
    return stats
```

### Performance Metrics

**1. Response Time Tracking:**
```python
import time
from functools import wraps

def track_response_time(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        # Log performance
        logger.info({
            "function": func.__name__,
            "response_time": end_time - start_time,
            "timestamp": datetime.now().isoformat()
        })
        
        return result
    return wrapper
```

**2. Memory Usage Monitoring:**
```bash
# ‡πÉ‡∏ô production script
import psutil
import GPUtil

def get_system_stats():
    return {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "gpu_utilization": GPUtil.getGPUs()[0].load * 100 if GPUtil.getGPUs() else 0,
        "disk_usage": psutil.disk_usage('/').percent
    }
```

---

## ‚úÖ Testing ‡πÅ‡∏•‡∏∞ Quality Assurance

### Unit Tests

**1. Test Langchain Service:**
```python
# tests/test_langchain_service.py
import pytest
from langchain_service.medical_ai import LangchainMedicalService

def test_service_initialization():
    service = LangchainMedicalService()
    assert service is not None
    assert service.vector_store is not None

def test_ask_question():
    service = LangchainMedicalService()
    result = service.ask_question("What is diabetes?")
    
    assert "answer" in result
    assert "confidence" in result
    assert result["confidence"] > 0
    assert len(result["answer"]) > 0

def test_add_knowledge():
    service = LangchainMedicalService()
    service.add_knowledge(
        "Test medical knowledge", 
        {"topic": "test", "category": "test"}
    )
    # Test if knowledge was added successfully
```

**2. Test FastAPI Endpoints:**
```python
# tests/test_fastapi.py
from fastapi.testclient import TestClient
from fastapi.app.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/health")
    assert response.status_code == 200
    assert "status" in response.json()

def test_medical_qa():
    response = client.post(
        "/api/medical-qa",
        json={"question": "What is diabetes?", "user_id": "test"}
    )
    assert response.status_code == 200
    assert "answer" in response.json()
```

### Integration Tests

**1. End-to-End Test:**
```bash
# tests/e2e_test.py
def test_full_pipeline():
    # 1. Start services
    # 2. Add knowledge
    # 3. Ask questions
    # 4. Verify responses
    # 5. Check database logs
    pass
```

**2. Load Testing:**
```python
# tests/load_test.py
import asyncio
import aiohttp

async def load_test(num_requests=100):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(num_requests):
            task = session.post(
                "http://localhost:8000/api/medical-qa",
                json={"question": f"Test question {i}", "user_id": f"user{i}"}
            )
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
        return responses
```

---

## üìà Best Practices

### 1. Security
- ‡πÉ‡∏ä‡πâ environment variables ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sensitive data
- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ CORS ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
- ‡πÉ‡∏ä‡πâ rate limiting
- Log access ‡πÅ‡∏•‡∏∞ errors

### 2. Performance
- ‡πÉ‡∏ä‡πâ model caching
- Implement batch processing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bulk operations
- Monitor memory usage
- Use async operations ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

### 3. Maintainability  
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô documentation ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
- ‡πÉ‡∏ä‡πâ type hints
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô unit tests
- Use logging ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

### 4. Scalability
- ‡πÉ‡∏ä‡πâ Docker ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö deployment
- Setup load balancing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production
- Use database connection pooling
- Monitor ‡πÅ‡∏•‡∏∞ optimize ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á

---

## üÜò ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç

**1. Langchain Service ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô:**
```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ dependencies
pip install -r requirements.txt

# ‡πÄ‡∏ä‡πá‡∏Ñ model files
ls -la models/

# ‡πÄ‡∏ä‡πá‡∏Ñ logs
tail -f logs/langchain.log
```

**2. FastAPI ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö:**
```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ service running
curl http://localhost:8000/health

# ‡πÄ‡∏ä‡πá‡∏Ñ port usage
netstat -an | grep 8000

# Restart service
pkill -f "uvicorn"
python fastapi/app/main.py
```

**3. N8N Workflow ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**
- ‡πÄ‡∏ä‡πá‡∏Ñ webhook URL
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö HTTP request settings
- ‡∏î‡∏π execution logs ‡πÉ‡∏ô N8N UI

**4. Memory Issues:**
```bash
# ‡∏•‡∏î model size
export TORCH_DTYPE=float16

# ‡πÉ‡∏ä‡πâ CPU-only mode
export CUDA_VISIBLE_DEVICES=""

# ‡πÄ‡∏û‡∏¥‡πà‡∏° swap space (Linux)
sudo swapon --show
```

‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?